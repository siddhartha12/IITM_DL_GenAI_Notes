{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563f1eb1-7147-4990-a8a7-7883c11a28a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# L2.1 - Universal Approximation Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d70c14-676f-436c-843e-ff25fa918c7d",
   "metadata": {},
   "source": [
    "1. Linear Regression: weighted sum of input features\n",
    "2. Neural network: has a linear and a nonlinear operation that helps warp modify and bend the sample space\n",
    "\n",
    "## Theorem\n",
    "A feedforward network with a single hidden layer and a non linear bounded, continuous activation can approximate any continuous function on a compact domain to arbitrary accuracy\n",
    "\n",
    "capacity result: existence of parameters for desired accuracy\n",
    "\n",
    "existence of parameters(alpha, w, b) for desired accuracy. Not a training guarantee, depth is not required by the theorem\n",
    "\n",
    "## Takeaways\n",
    "* Linear Regression captures only hyperplanes; inadequate for general nonlinear structure\n",
    "* Neural networks = linear maps + nonlinear activations => flexible function families\n",
    "* UAT: A single hidden layer has enough capacity to approximate any continuous function on a bounded set\n",
    "* Depth is about efficiency and inductive bias: fewer units, better compositional structure\n",
    "* Theorem guarentees representability, not learnability or generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d4794-0cf5-4c02-9a04-109ae309b4a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# L2.2 - Forward Pass and Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4af7389-48cc-4178-8004-355786b3f1f8",
   "metadata": {},
   "source": [
    "## Foward Pass\n",
    "Process of taking an input vector x and feeding it through the network layer by layer, to produce a final prediction y_hat\n",
    "\n",
    "Computation for every layer follows the same two-step, vectorized pattern:\n",
    "* Linear Combination:\n",
    "  Calculate the weighter sum for all neurons in layer\n",
    "  z = Wa + b\n",
    "* Non-linear Activation\n",
    "  Apply the activation function element wise\n",
    "  a = G(z)\n",
    "\n",
    "## Loss functions\n",
    "L(y, y_hat)\n",
    "\n",
    "* For Regression Tasks: Mean Squared Error\n",
    "* For Classification Tasks: Cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a9a318-da89-4e90-9028-c07a55307c98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# L2.3 - Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4962917b-529b-4020-83e2-ca50c0b14c55",
   "metadata": {},
   "source": [
    "**Goal:** Find the parameters theta that minimizes the Loss Function L(theta) -> Means find the lowest point in the landscape\n",
    "\n",
    "**Update rule:** At each step, we move in the direction of the steepest descent\n",
    "\n",
    "## Local vs Global Minima\n",
    "* global minima: true lowest point, optimal solution\n",
    "* Local MinimaL: valley that traps the algorithms\n",
    "\n",
    "## Types of Gradient Descent\n",
    "1. Batch GD\n",
    "    * Entire Dataset\n",
    "    * Once per epoch\n",
    "    * Slow\n",
    "    * Smooth and Stable\n",
    "    * Inefficient for large data\n",
    "    * Memory High\n",
    "2. Stochastic DG (SGD)\n",
    "    * Single Data point\n",
    "    * After each data point\n",
    "    * Fast per iteration\n",
    "    * Fast (per iteration)\n",
    "    * Noisy and erratic\n",
    "    * Efficient\n",
    "    * Memory Low\n",
    "3. Mini-Betch GD\n",
    "    * After each data point\n",
    "    * Moderate\n",
    "    * Smooth convergence\n",
    "    * Very Efficient on GPU\n",
    "    * Memory Moderate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c614ea2-cff8-449b-aabc-5cfb8e137545",
   "metadata": {},
   "source": [
    "# L2.4 to L2.11 - Customer Churn Prediction\n",
    "https://colab.research.google.com/drive/1J4Hv6V5jwrpyuxNCKr5NRw9bm5Z_IASr#scrollTo=BmBFFMoD5N-U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc7a57-89ca-4f23-8c70-6c1dd8949e63",
   "metadata": {},
   "source": [
    "# L2.12 & 13 - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9795aad5-d984-4a38-897f-31625f64e6cd",
   "metadata": {},
   "source": [
    "## Chain rule\n",
    "chain rule provides the effect of how the parameter (w1) affect the final loss at the end of a long chain of computations\n",
    "\n",
    "in vector cases y = f(x) and L = g(y) the derivatives become jacobian matrices and chain rule becomes matrix multiplication -> this is the foundation of neural networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896bd344-cf85-492c-87e6-b198a62be661",
   "metadata": {},
   "source": [
    "# L2.14 & 15 - XOR problem\n",
    "* Exclusive XOR function is a classic problem in machine learning because it is not linearly separable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0f7d8-5075-49b1-8751-ba4369fa45e4",
   "metadata": {},
   "source": [
    "# L2.16 - Fashion MNIST - Overview and Data Procession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e805c0-b7e1-4839-83ad-da5a298e310f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
